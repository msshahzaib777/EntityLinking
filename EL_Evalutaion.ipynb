{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "OVERLAP = 10\n",
    "TOP_K = 500\n",
    "BATCH_SIZE = 64\n",
    "DEGUB = False\n",
    "THRESHOLD = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# model_checkpoint = \"Mini_LCQUAD\\checkpoint-500\"\n",
    "model_checkpoint = \"Mini_EL2\\checkpoint-500\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, handle_impossible_answer=True, batch_size=BATCH_SIZE, device=device )\n",
    "model_checkpoint = \"Mini_LCQUAD2\\checkpoint-500\"\n",
    "question_answerer2 = pipeline(\"question-answering\", model=model_checkpoint, handle_impossible_answer=True, batch_size=BATCH_SIZE, device=device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(text, entity_ids = []):\n",
    "    response = requests.get(\"https://qanswer-core1.univ-st-etienne.fr/api/entitylinker\" , params={'text': text, 'language': 'en', 'knowledgebase': 'wikidata'})\n",
    "    input_entities = []\n",
    "    for r in response.json():\n",
    "        if 'uri' in r and 'http://www.wikidata.org/entity/' in r['uri']:\n",
    "            id = r['uri'].replace('http://www.wikidata.org/entity/','')\n",
    "            if(\"wd:\" + str(id) in entity_ids):\n",
    "                entity = True\n",
    "            else:\n",
    "                entity = False\n",
    "            input_entities.append({ \"start\":r['start'], \"end\":r['end'], \"text\": r['text'], \"id\": id, \"description\": str(r[\"qaContext\"][\"disambiguation\"] or ''), \"entity\": entity  }) \n",
    "    return input_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENTQA(context, inputs):\n",
    "    questions = [x[\"text\"] + \" : \" + x[\"description\"] for x in inputs]\n",
    "    start_time = time.time()\n",
    "\n",
    "    df = pd.DataFrame.from_records({\"question\": questions, \"context\": context})\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    ans = question_answerer(dataset)\n",
    "    if(DEGUB): print(\"Linking Time: --- %s seconds ---\" % (time.time() - start_time))\n",
    "    # \n",
    "    for i in range(0, len(questions)):\n",
    "        ans[i][\"question\"] = questions[i]\n",
    "        ans[i][\"id\"] = inputs[i][\"id\"]\n",
    "        ans[i][\"start\"] = inputs[i][\"start\"]\n",
    "        ans[i][\"end\"] = inputs[i][\"end\"]\n",
    "    ans = [{\"question\":x[\"question\"],\n",
    "            \"ans\": x[\"answer\"],\n",
    "            \"start\": x[\"start\"],\n",
    "            \"end\": x[\"end\"],\n",
    "            \"score\": x[\"score\"],\n",
    "            \"QId\":  x[\"id\"]\n",
    "            } for x in ans if(x[\"answer\"] != '')]\n",
    "    return {\"context\": context, \"results\": ans} \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(text, max_len, overlap=0):\n",
    "    words = text.split()\n",
    "    segments = []\n",
    "    current_seg = \"\"\n",
    "    seg_len = 0\n",
    "    wordCount = 0\n",
    "    while wordCount  <  len(words):\n",
    "        if(seg_len < max_len):\n",
    "            current_seg += \" \"+words[wordCount]\n",
    "            seg_len += 1\n",
    "            wordCount += 1\n",
    "        else:\n",
    "            segments.append(current_seg.strip())\n",
    "            current_seg = \"\"\n",
    "            seg_len = 0\n",
    "            if(len(words)-wordCount+overlap < max_len):\n",
    "                wordCount = len(words)-max_len\n",
    "            elif(wordCount-overlap > 0):\n",
    "                wordCount -= overlap\n",
    "\n",
    "    segments.append(current_seg.strip())\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropOverlapping(entities):\n",
    "    to_drop = set()\n",
    "    if(DEGUB): print(\"Total Entities: \", len(entities))\n",
    "    # removing duplicate and ovelapping candidate\n",
    "    for i in  range(0, len(entities)):\n",
    "            if (i in to_drop):\n",
    "                continue\n",
    "            if(entities[i][\"score\"]*100 < THRESHOLD):\n",
    "                to_drop.add(i)\n",
    "                continue\n",
    "            for j in  range(i, len(entities)):\n",
    "                if (j in to_drop):\n",
    "                    continue\n",
    "                if(entities[i][\"start\"] < entities[j][\"end\"]) and (entities[i][\"end\"] > entities[j][\"start\"]):\n",
    "                    if(entities[i][\"score\"] > entities[j][\"score\"]):   \n",
    "                        to_drop.add(j)\n",
    "                    elif(entities[i][\"score\"] < entities[j][\"score\"]):\n",
    "                        to_drop.add(i)\n",
    "                if(entities[i][\"start\"] == entities[j][\"start\"]) and (entities[i][\"end\"] == entities[j][\"end\"]):\n",
    "                    if(entities[i][\"score\"] > entities[j][\"score\"]):   \n",
    "                        to_drop.add(j)\n",
    "                    elif(entities[i][\"score\"] < entities[j][\"score\"]):\n",
    "                        to_drop.add(i)\n",
    "    to_drop = list(to_drop)\n",
    "    to_drop.sort()\n",
    "    for i in range(0,len(to_drop)):\n",
    "        del entities[to_drop[i]-i]\n",
    "    return entities\n",
    "\n",
    "def mergeResults(results):\n",
    "    results_merged  = { \"text\": \"\",\n",
    "                    \"entities\": []}\n",
    "    results_merged[\"text\"] = results[0][\"context\"]\n",
    "    # results_merged[\"entities\"].extend(mapIndexes(0, 0, results[0][\"results\"]))\n",
    "    results_merged[\"entities\"].extend(results[0][\"results\"]) \n",
    "    for i in range(1, len(results)-1):\n",
    "        len_overlap = len(\" \".join(results[i][\"context\"].split()[:OVERLAP]))\n",
    "        len_seg = len(results_merged[\"text\"])\n",
    "        results_merged[\"text\"] = results_merged[\"text\"] + results[i][\"context\"][OVERLAP:]\n",
    "        # results_merged[\"entities\"].extend(mapIndexes(len_seg, len_overlap, results[i][\"results\"]))\n",
    "        results_merged[\"entities\"].extend(results[i][\"results\"])\n",
    "\n",
    "    index = results[-1][\"context\"].index(results[-2][\"context\"][-20:]) + 20\n",
    "    len_seg = len(results_merged[\"text\"])\n",
    "    results_merged[\"text\"] = results_merged[\"text\"] + results[-1][\"context\"][index:]\n",
    "    # results_merged[\"entities\"].extend(mapIndexes(len_seg, index, results[-1][\"results\"]))\n",
    "    results_merged[\"entities\"].extend(results[-1][\"results\"])\n",
    "\n",
    "    return dropOverlapping(results_merged[\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loopENTQA(text):\n",
    "    answers = []\n",
    "    if(DEGUB): print(\"Token Counts: --- %s tokens ---\" % (len(text.split())))\n",
    "    start_time = time.time()\n",
    "    # Expand the Text and fetch the candidates\n",
    "    inputs = expand(text)\n",
    "    if(DEGUB): print(\"Expending Time: --- %s seconds ---\" % (time.time() - start_time))\n",
    "    if(DEGUB): print(\"Candidate Entities (whole Text):\", len(inputs))\n",
    "    start_time = time.time()\n",
    "    segments = segment_text(text, MAX_LEN, OVERLAP)\n",
    "    # segment text with overlapping\n",
    "    if(DEGUB): print(\"Segmentation Time: --- %s seconds ---\" % (time.time() - start_time))\n",
    "    previous_word_count = -1\n",
    "    for i in range(0, len(segments)):\n",
    "        candidates = []\n",
    "        if(i == len(segments)-1):\n",
    "            # select candidates in the specific segment\n",
    "            candidates = [x for x in inputs if(x[\"end\"] >= len(text.split()[-MAX_LEN:]) )]\n",
    "        else:\n",
    "            word_count = len(segments[i].split())\n",
    "            candidates = [x for x in inputs if((x[\"start\"] > previous_word_count)  and (x[\"end\"] <= previous_word_count + word_count))]\n",
    "            previous_word_count += word_count -1\n",
    "        \n",
    "        if(len(segments) == 1):\n",
    "            candidates = inputs\n",
    "        if(DEGUB): print(\"Candidate Entities (segment Text):\", len(candidates))\n",
    "        # received the positive candidates\n",
    "        answers.append(ENTQA(segments[i], candidates))\n",
    "    start_time = time.time()\n",
    "    results = {\"text\":  text}\n",
    "    if(len(segments)> 1):\n",
    "        # merging the segments\n",
    "        results[\"entities\"] = mergeResults(answers)\n",
    "    else:\n",
    "        results[\"entities\"] =  dropOverlapping(answers[-1][\"results\"])\n",
    "    \n",
    "    if(DEGUB): print(\"Merging Time: --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEGUB = True\n",
    "# text = \"The project “RE-ODRA – social and economic activation of post-factory areas in Nowa Sól poland is related to revitalisation of the industrial areas of the former Odra factory together with the immediate surroundings, as part of Delta airline the tasks set out in the Local Programme for the Regeneration of the state of New York for the years 2016-2023.  Which female actress is the voice over on South Park American animated sitcom 48.67 and is employed as a singer? What periodical literature does Delta Air Lines use as a moutpiece? Delta Air Lines was the first airline to use Boeing 747. Delta went bankcrupt after the financial crisis in 2005 in state of New York.\"\n",
    "# text = \"Nowa Sól town on the Oder River in Lubusz Voivodeship, western Poland\"\n",
    "text = \"\"\"Borgomasino is a comune (municipality) in the Metropolitan City of Turin in the Italian region Piedmont, located about 40 kilometres (25 mi) northeast of Turin.\n",
    "# Among the sites are the Parish Church of Santissimo Salvatore designed by Bernardo Vittone and the castle.\"\"\"\n",
    "# text = \"\"\"Leonardo di ser Piero da Vinci [b] (15 April 1452 – 2 May 1519) was an Italian polymath of the High Renaissance who was active as a painter, draughtsman, engineer, scientist, theorist, sculptor, and architect.[3] While his fame initially rested on his achievements as a painter, he also became known for his notebooks, in which he made drawings and notes on a variety of subjects, including anatomy, astronomy, botany, cartography, painting, and paleontology. Leonardo is widely regarded to have been a genius who epitomized the Renaissance humanist ideal,[4] and his collective works comprise a contribution to later generations of artists matched only by that of his younger contemporary, Michelangelo.\"\"\"\n",
    "# text = \"Leonardo di ser Piero da Vinci\"\n",
    "# text = \"draughtsman engineer scientist theorist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Counts: --- 43 tokens ---\n",
      "Expending Time: --- 0.7468669414520264 seconds ---\n",
      "Candidate Entities (whole Text): 116\n",
      "Segmentation Time: --- 0.0 seconds ---\n",
      "Candidate Entities (segment Text): 116\n",
      "Linking Time: --- 0.41979360580444336 seconds ---\n",
      "Merging Time: --- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Mini_EL2\\checkpoint-500\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint, handle_impossible_answer=False, batch_size=BATCH_SIZE, device=device )\n",
    "THRESHOLD = 50\n",
    "results = loopENTQA(text)\n",
    "results[\"entities\"] = sorted(results[\"entities\"], key=lambda d: d['start']) \n",
    "display_text = []\n",
    "text_splitted =  results[\"text\"].replace('-', ' ').replace(',', ' ').replace('.', ' ').split()\n",
    "last_index = 0\n",
    "for i in results[\"entities\"]:\n",
    "    display_text.append(\" \" + \" \".join(text_splitted[last_index:i[\"start\"]]) + \" \")\n",
    "    display_text.append((\" \".join(text_splitted[i[\"start\"]:i[\"end\"]]), i[\"question\"].split(\":\")[-1] + \" \" + \"{:.2f}\".format(i[\"score\"]*100) , \"#faa\", \"#000000\"))\n",
    "    last_index = i[\"end\"]\n",
    "display_text.append(\" \" + \" \".join(text_splitted[last_index:])) \n",
    "display_text.append(\"\\n\")\n",
    "with open('display_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(display_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Counts: --- 43 tokens ---\n",
      "Expending Time: --- 0.6230158805847168 seconds ---\n",
      "Candidate Entities (whole Text): 116\n",
      "Segmentation Time: --- 0.0 seconds ---\n",
      "Candidate Entities (segment Text): 116\n",
      "Linking Time: --- 0.3925018310546875 seconds ---\n",
      "Merging Time: --- 0.001026153564453125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Mini_LCQUAD2\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint, handle_impossible_answer=False, batch_size=BATCH_SIZE, device=device )\n",
    "THRESHOLD = 20\n",
    "results = loopENTQA(text)\n",
    "results[\"entities\"] = sorted(results[\"entities\"], key=lambda d: d['start']) \n",
    "display_text = []\n",
    "text_splitted =  results[\"text\"].replace('-', ' ').replace(',', ' ').replace('.', ' ').split()\n",
    "last_index = 0\n",
    "for i in results[\"entities\"]:\n",
    "    display_text.append(\" \" + \" \".join(text_splitted[last_index:i[\"start\"]]) + \" \")\n",
    "    display_text.append((\" \".join(text_splitted[i[\"start\"]:i[\"end\"]]), i[\"question\"].split(\":\")[-1] + \" \" + \"{:.2f}\".format(i[\"score\"]*100) , \"#faa\", \"#000000\"))\n",
    "    last_index = i[\"end\"]\n",
    "display_text.append(\" \" + \" \".join(text_splitted[last_index:])) \n",
    "display_text.append(\"\\n\")\n",
    "with open('display_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(display_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
